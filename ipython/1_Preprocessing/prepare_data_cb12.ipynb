{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CareerBuilder 2012\n",
    "\n",
    "For the CareerBuilder 2012 dataset we first need to artificially create sessions out of the user internactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sessions(data, \n",
    "                  session_th=30 * 60, \n",
    "                  is_ordered=False, \n",
    "                  user_key='user_id', \n",
    "                  item_key='item_id', \n",
    "                  time_key='ts'):\n",
    "    \"\"\"Assigns session ids to the events in data without grouping keys\"\"\"\n",
    "    if not is_ordered:\n",
    "        # sort data by user and time\n",
    "        data.sort_values(by=[user_key, time_key], ascending=True, inplace=True)\n",
    "    # compute the time difference between queries\n",
    "    tdiff = np.diff(data[time_key].values)\n",
    "    # check which of them are bigger then session_th\n",
    "    split_session = tdiff > session_th\n",
    "    split_session = np.r_[True, split_session]\n",
    "    # check when the user chenges is data\n",
    "    new_user = data['user_id'].values[1:] != data['user_id'].values[:-1]\n",
    "    new_user = np.r_[True, new_user]\n",
    "    # a new sessions stars when at least one of the two conditions is verified\n",
    "    new_session = np.logical_or(new_user, split_session)\n",
    "    # compute the session ids\n",
    "    session_ids = np.cumsum(new_session)\n",
    "    data['session_id'] = session_ids\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set\n",
    "\n",
    "A test set can be either created by (1) adding the last session of every user to be tested or, (2) making a time-based split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_session_out_split(data,\n",
    "                           user_key='user_id',\n",
    "                           item_key='item_id',\n",
    "                           session_key='session_id',\n",
    "                           time_key='ts',\n",
    "                           clean_test=True,\n",
    "                           min_session_length=2):\n",
    "    \"\"\"\n",
    "    last-session-out split\n",
    "    assign the last session of every user to the test set and the remaining ones to the training set\n",
    "    \"\"\"\n",
    "    sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key]\n",
    "    last_session = sessions.last()\n",
    "    train = data[~data.session_id.isin(last_session.values)].copy()\n",
    "    test = data[data.session_id.isin(last_session.values)].copy()\n",
    "    if clean_test:\n",
    "        train_items = train[item_key].unique()\n",
    "        test = test[test[item_key].isin(train_items)]\n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        slen = test[session_key].value_counts()\n",
    "        good_sessions = slen[slen >= min_session_length].index\n",
    "        test = test[test[session_key].isin(good_sessions)].copy()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_n_days_out_split(data, n=1,\n",
    "                          user_key='user_id',\n",
    "                          item_key='item_id',\n",
    "                          session_key='session_id',\n",
    "                          time_key='ts',\n",
    "                          clean_test=True,\n",
    "                          min_session_length=2):\n",
    "    \"\"\"\n",
    "    last n-days out split\n",
    "    assign the sessions in the last n days to the test set and remaining to the training one\n",
    "    \"\"\"\n",
    "    DAY = 24 * 60 * 60\n",
    "    data.sort_values(by=[user_key, time_key], inplace=True)\n",
    "    # start times of all sessions\n",
    "    #sessions_start = data.groupby(session_key)[time_key].agg('min')\n",
    "    # extract test start and end time\n",
    "    end_time = data[time_key].max()\n",
    "    test_start = end_time - n * DAY\n",
    "    \n",
    "    # get train and test indicies\n",
    "    session_max_times = data.groupby(session_key)[time_key].max()\n",
    "    session_train = session_max_times[session_max_times < test_start].index\n",
    "    session_test = session_max_times[session_max_times >= test_start].index\n",
    "    \n",
    "    # in1d: Returns a boolean array the same length as ar1 that is True where \n",
    "    # an element of ar1 is in ar2 and False otherwise.\n",
    "    train = data[\n",
    "        np.in1d(\n",
    "            data[session_key], \n",
    "            session_train\n",
    "        )\n",
    "    ].copy()\n",
    "    test = data[\n",
    "        np.in1d(\n",
    "            data[session_key], \n",
    "            session_test\n",
    "        )\n",
    "    ].copy()\n",
    "\n",
    "    #train = data[data.session_id.isin(sessions_start[sessions_start < test_start].index)].copy()\n",
    "    #test = data[data.session_id.isin(sessions_start[sessions_start >= test_start].index)].copy()\n",
    "    if clean_test:\n",
    "        before_items = len(test[item_key].unique())\n",
    "        # remove items which do not occur in the test set\n",
    "        test = test[np.in1d(test[item_key], train[item_key])]\n",
    "        after_items = len(test[item_key].unique())\n",
    "        print(\"Before item count: \" + str(before_items))\n",
    "        print(\"After item count: \" + str(after_items))\n",
    "        \n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        \n",
    "        tslength = test.groupby(session_key).size()\n",
    "        test = test[\n",
    "           np.in1d(\n",
    "                test[session_key], \n",
    "                tslength[tslength >= min_session_length].index\n",
    "            )\n",
    "        ].copy()\n",
    "    \n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Career Builder 12 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../../data/\"\n",
    "dataset = \"cb12/\"\n",
    "\n",
    "raw_path = path + dataset + \"raw/\" \n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Kaggle Career Builder 2012 dataset:\n",
    "* Only have the **application** interaction is available\n",
    "\n",
    "Sessions are partitioned by a **30-minute** idle time\n",
    "\n",
    "Keep all sessions: users with >= 1 sessions and also overly active ones (< 200,000 sessions)\n",
    "\n",
    "Link to dataset: https://www.kaggle.com/c/job-recommendation/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>1333554983</td>\n",
       "      <td>169528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>1333674180</td>\n",
       "      <td>284009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>1333593627</td>\n",
       "      <td>2121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>1333593422</td>\n",
       "      <td>848187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>1333665846</td>\n",
       "      <td>733748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  created_at  item_id  interaction_type\n",
       "0       47  1333554983   169528                 0\n",
       "1       47  1333674180   284009                 0\n",
       "2       47  1333593627     2121                 0\n",
       "3       47  1333593422   848187                 0\n",
       "4       47  1333665846   733748                 0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = pd.read_csv(raw_path + \"apps.tsv\", header=0, sep='\\t')\n",
    "interactions = interactions.rename(columns={\"UserID\": \"user_id\", \"JobID\": \"item_id\", \"ApplicationDate\": \"created_at\"})\n",
    "interactions[\"interaction_type\"] = 0 # no different interactions\n",
    "interactions = interactions.drop(columns=[\"WindowID\", \"Split\"])\n",
    "interactions['created_at'] = interactions['created_at'].astype(\"datetime64[ms]\").astype(np.int64) // 10**9\n",
    "interactions.to_csv(raw_path + \"interactions.csv\", sep='\\t')\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2012-04-01 00:00:21\n",
      "Start Time: 2012-06-26 23:59:55\n",
      "Building sessions\n",
      "        user_id  created_at  item_id  interaction_type  session_id\n",
      "563238        7  1335082466   309823                 0           1\n",
      "563239        7  1335615478   703889                 0           2\n",
      "780806        9  1337034642   809208                 0           3\n",
      "Original data:\n",
      "Num items: 354303\n",
      "Num users: 321235\n",
      "Num sessions: 639534\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 213445\n",
      "Num users: 91547\n",
      "Num sessions: 123746\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Time: {}\".format(pd.to_datetime(interactions[\"created_at\"].min(), unit=\"s\")))\n",
    "print(\"Start Time: {}\".format(pd.to_datetime(interactions[\"created_at\"].max(), unit=\"s\")))\n",
    "\n",
    "# remove NaN values\n",
    "interactions = interactions[np.isfinite(interactions['created_at'])]\n",
    "# convert back to long from float\n",
    "interactions['created_at'] = interactions['created_at'].astype(np.int64)\n",
    "\n",
    "\n",
    "interactions['interaction_type'] = interactions['interaction_type'].fillna(0).astype('int')\n",
    "\n",
    "\n",
    "print('Building sessions')\n",
    "# partition interactions into sessions with 30-minutes idle time\n",
    "interactions = make_sessions(interactions, session_th=30 * 60, time_key='created_at', is_ordered=False)\n",
    "\n",
    "\n",
    "print(interactions.head(3))\n",
    "# drop duplicate interactions\n",
    "interactions = interactions.drop_duplicates(['session_id','created_at'])\n",
    "\n",
    "print('Original data:')\n",
    "print('Num items: {}'.format(interactions.item_id.nunique()))\n",
    "print('Num users: {}'.format(interactions.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(interactions.session_id.nunique()))\n",
    "\n",
    "print('Filtering data')\n",
    "# drop duplicate interactions within the same session\n",
    "interactions.drop_duplicates(subset=['item_id', 'session_id', 'interaction_type'], keep='first', inplace=True)\n",
    "\n",
    "# keep items with >=1 interactions\n",
    "item_pop = interactions.item_id.value_counts()\n",
    "#good_items = item_pop[item_pop >= 5].index\n",
    "good_items = item_pop[item_pop >= 1].index\n",
    "\n",
    "inter_dense = interactions[interactions.item_id.isin(good_items)]\n",
    "\n",
    "# remove sessions with length < 3\n",
    "session_length = inter_dense.session_id.value_counts()\n",
    "good_sessions = session_length[session_length >= 3].index\n",
    "inter_dense = inter_dense[inter_dense.session_id.isin(good_sessions)]\n",
    "\n",
    "# let's keep only returning users (with >= 5 sessions) and remove overly active ones (>=200 sessions)\n",
    "sess_per_user = inter_dense.groupby('user_id')['session_id'].nunique()\n",
    "good_users = sess_per_user[(sess_per_user >= 1) & (sess_per_user < 200000)].index\n",
    "inter_dense = inter_dense[inter_dense.user_id.isin(good_users)]\n",
    "print('Filtered data:')\n",
    "print('Num items: {}'.format(inter_dense.item_id.nunique()))\n",
    "print('Num users: {}'.format(inter_dense.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(inter_dense.session_id.nunique()))\n",
    "\n",
    "inter_dense.to_csv(interim_path + \"interactions.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create train and test set by doing a time-based (2 weeks) split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning data\n",
      "Before item count: 13950\n",
      "After item count: 9747\n",
      "Before item count: 21225\n",
      "After item count: 11935\n"
     ]
    }
   ],
   "source": [
    "print('Partitioning data')\n",
    "# last-session-out partitioning\n",
    "train_full_sessions, test_sessions = last_n_days_out_split(inter_dense, n=14,\n",
    "                                                            user_key='user_id',\n",
    "                                                            item_key='item_id',\n",
    "                                                            session_key='session_id',\n",
    "                                                            time_key='created_at',\n",
    "                                                            clean_test=True)\n",
    "train_valid_sessions, valid_sessions = last_n_days_out_split(train_full_sessions, n=14,\n",
    "                                                              user_key='user_id',\n",
    "                                                              item_key='item_id',\n",
    "                                                              session_key='session_id',\n",
    "                                                              time_key='created_at',\n",
    "                                                              clean_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Sessions: 66348\n",
      "Testing - Sessions: 9632\n",
      "Train + Test - Sessions: 75980\n",
      "Training - Items: 60916\n",
      "Testing - Items: 9632\n",
      "Train + Test - Items: 60916\n",
      "Train Validating - Sessions: 51717\n",
      "Test Validating - Sessions: 10574\n"
     ]
    }
   ],
   "source": [
    "# print statistics\n",
    "\n",
    "train_len = len(train_full_sessions.session_id.unique())\n",
    "train_item_len = len(train_full_sessions.item_id.unique())\n",
    "\n",
    "test_len = len(test_sessions.session_id.unique())\n",
    "test_item_len = len(test_sessions.item_id.unique())\n",
    "\n",
    "merged_items = train_full_sessions.append(test_sessions, ignore_index=True)\n",
    "merged_item_len = len(merged_items.item_id.unique())\n",
    "\n",
    "print(\"Training - Sessions: \" + str(train_len))\n",
    "print(\"Testing - Sessions: \" + str(test_len))\n",
    "print(\"Train + Test - Sessions: \" + str(train_len + test_len))\n",
    "\n",
    "print(\"Training - Items: \" + str(train_item_len))\n",
    "print(\"Testing - Items: \" + str(test_len))\n",
    "print(\"Train + Test - Items: \" + str(merged_item_len))\n",
    "\n",
    "\n",
    "print(\"Train Validating - Sessions: \" + str(len(train_valid_sessions.session_id.unique())))\n",
    "print(\"Test Validating - Sessions: \" + str(len(valid_sessions.session_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Store train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_sessions.to_csv(processed_path + \"train_14d.csv\", sep='\\t')\n",
    "test_sessions.to_csv(processed_path + \"test_14d.csv\", sep='\\t')\n",
    "train_valid_sessions.to_csv(processed_path + \"valid_train_14d.csv\", sep='\\t')\n",
    "valid_sessions.to_csv(processed_path + \"valid_test_14d.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create train and test session vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60916\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary from train set\n",
    "unqiue_train_items = train_full_sessions.item_id.unique()\n",
    "# store (or load)\n",
    "unqiue_train_items_df = pd.DataFrame(unqiue_train_items, columns=[\"item_id\"])\n",
    "print(len(unqiue_train_items_df))\n",
    "#unqiue_train_items_df.to_csv(interim_path + 'vocabulary_min_5_app.csv', header=True)\n",
    "unqiue_train_items_df.to_csv(interim_path + 'vocabulary.csv', header=True)\n",
    "unqiue_train_items_df = pd.read_csv(interim_path + 'vocabulary.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unqiue_train_items_dict = unqiue_train_items_df.to_dict('dict')[\"item_id\"]\n",
    "# inverse that item_id is key and index is value\n",
    "unqiue_train_items_dict_inv = {v: k for k, v in unqiue_train_items_dict.items()}\n",
    "\n",
    "# session_vectors = []\n",
    "# session_groups = train_full_sessions.groupby(\"session_id\")\n",
    "session_ids = train_full_sessions.session_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108783 sessions to encode.\n",
      "107783 sessions remaining to encode.\n",
      "106783 sessions remaining to encode.\n",
      "105783 sessions remaining to encode.\n",
      "104783 sessions remaining to encode.\n",
      "103783 sessions remaining to encode.\n",
      "102783 sessions remaining to encode.\n",
      "101783 sessions remaining to encode.\n",
      "100783 sessions remaining to encode.\n",
      "99783 sessions remaining to encode.\n",
      "98783 sessions remaining to encode.\n",
      "97783 sessions remaining to encode.\n",
      "96783 sessions remaining to encode.\n",
      "95783 sessions remaining to encode.\n",
      "94783 sessions remaining to encode.\n",
      "93783 sessions remaining to encode.\n",
      "92783 sessions remaining to encode.\n",
      "91783 sessions remaining to encode.\n",
      "90783 sessions remaining to encode.\n",
      "89783 sessions remaining to encode.\n",
      "88783 sessions remaining to encode.\n",
      "87783 sessions remaining to encode.\n",
      "86783 sessions remaining to encode.\n",
      "85783 sessions remaining to encode.\n",
      "84783 sessions remaining to encode.\n",
      "83783 sessions remaining to encode.\n",
      "82783 sessions remaining to encode.\n",
      "81783 sessions remaining to encode.\n",
      "80783 sessions remaining to encode.\n",
      "79783 sessions remaining to encode.\n",
      "78783 sessions remaining to encode.\n",
      "77783 sessions remaining to encode.\n",
      "76783 sessions remaining to encode.\n",
      "75783 sessions remaining to encode.\n",
      "74783 sessions remaining to encode.\n"
     ]
    }
   ],
   "source": [
    "print(str(len(session_ids)) + \" sessions to encode.\")\n",
    "s_counter = 0\n",
    "file_name = interim_path + 'train_session_interaction_vector.csv'\n",
    "with open(file_name, \"w\") as file:\n",
    "    header = \",\".join(map(str, range(len(unqiue_train_items))))\n",
    "    file.write(header + \"\\n\")\n",
    "    for session_id in session_ids:\n",
    "        session_group = train_full_sessions[train_full_sessions[\"session_id\"] == session_id]\n",
    "        # vector length = len(unqiue_train_items)\n",
    "        session_vector = np.zeros((len(unqiue_train_items),), dtype=int)\n",
    "        # fill 1s for session items\n",
    "        for index, row in session_group.iterrows():\n",
    "            item_index = unqiue_train_items_dict_inv[row[\"item_id\"]]\n",
    "            # 1-hot encode\n",
    "            session_vector[item_index] = 1\n",
    "        # append session vector\n",
    "        session_vector_with_index = np.insert(session_vector, 0, s_counter)\n",
    "        session_vector_with_index_string = \",\".join(map(str,session_vector_with_index))\n",
    "        file.write(session_vector_with_index_string + \"\\n\")\n",
    "        s_counter += 1\n",
    "        if (s_counter % 1000 == 0):\n",
    "            print(str(len(session_ids) - s_counter) + \" sessions remaining to encode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "interactions.interaction_type.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Num items: 197590\n",
      "Train Num sessions: 108783\n",
      "Train Num events: 599946\n",
      "Test Num items: 13361\n",
      "Test Num sessions: 11364\n",
      "Test Num events: 61964\n"
     ]
    }
   ],
   "source": [
    "print('Train Num items: {}'.format(train_full_sessions.item_id.nunique()))\n",
    "print('Train Num sessions: {}'.format(train_full_sessions.session_id.nunique()))\n",
    "print('Train Num events: {}'.format(len(train_full_sessions)))\n",
    "print('Test Num items: {}'.format(test_sessions.item_id.nunique()))\n",
    "print('Test Num sessions: {}'.format(test_sessions.session_id.nunique()))\n",
    "print('Test Num events: {}'.format(len(test_sessions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sessions\n",
      "        Unnamed: 0  user_id  created_at  item_id  interaction_type  session_id\n",
      "563238      563238        7  1335082466   309823                 0           1\n",
      "563239      563239        7  1335615478   703889                 0           2\n",
      "780806      780806        9  1337034642   809208                 0           3\n",
      "Original data:\n",
      "Num items: 365668\n",
      "Num users: 321235\n",
      "Num sessions: 639534\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 233737\n",
      "Num users: 100995\n",
      "Num sessions: 139379\n",
      "Partitioning data\n",
      "Before item count: 29906\n",
      "After item count: 14241\n",
      "Before item count: 44612\n",
      "After item count: 17775\n",
      "Data - Sessions: 139379\n",
      "Training - Sessions: 124222\n",
      "Testing - Sessions: 11743\n",
      "Train Validating - Sessions: 104016\n",
      "Test Validating - Sessions: 13088\n",
      "Train Num items: 218072\n",
      "Train Num sessions: 124222\n",
      "Train Num events: 892697\n",
      "Test Num items: 13931\n",
      "Test Num sessions: 11743\n",
      "Test Num events: 66726\n"
     ]
    }
   ],
   "source": [
    "interactions = pd.read_csv(\"../../data/cb12/raw/interactions.csv\", header=0, sep='\\t')\n",
    "\n",
    "# remove NaN values\n",
    "interactions = interactions[np.isfinite(interactions['created_at'])]\n",
    "# convert back to long from float\n",
    "interactions['created_at'] = interactions['created_at'].astype(np.int64)\n",
    "\n",
    "interactions['interaction_type'] = interactions['interaction_type'].fillna(0).astype('int')\n",
    "\n",
    "print('Building sessions')\n",
    "# partition interactions into sessions with 30-minutes idle time\n",
    "interactions = make_sessions(interactions, session_th=30 * 60, time_key='created_at', is_ordered=False)\n",
    "\n",
    "\n",
    "print(interactions.head(3))\n",
    "# drop duplicate interactions\n",
    "interactions = interactions.drop_duplicates(['item_id','session_id','created_at'])\n",
    "\n",
    "print('Original data:')\n",
    "print('Num items: {}'.format(interactions.item_id.nunique()))\n",
    "print('Num users: {}'.format(interactions.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(interactions.session_id.nunique()))\n",
    "\n",
    "print('Filtering data')\n",
    "# keep items with >=20 interactions\n",
    "item_pop = interactions.item_id.value_counts()\n",
    "good_items = item_pop[item_pop >= 1].index\n",
    "inter_dense = interactions[interactions.item_id.isin(good_items)]\n",
    "# remove sessions with length < 3\n",
    "session_length = inter_dense.session_id.value_counts()\n",
    "good_sessions = session_length[session_length >= 3].index\n",
    "inter_dense = inter_dense[inter_dense.session_id.isin(good_sessions)]\n",
    "# let's keep only returning users (with >= 5 sessions) and remove overly active ones (>=200 sessions)\n",
    "sess_per_user = inter_dense.groupby('user_id')['session_id'].nunique()\n",
    "good_users = sess_per_user[(sess_per_user >= 1) & (sess_per_user < 200000)].index\n",
    "inter_dense = inter_dense[inter_dense.user_id.isin(good_users)]\n",
    "print('Filtered data:')\n",
    "print('Num items: {}'.format(inter_dense.item_id.nunique()))\n",
    "print('Num users: {}'.format(inter_dense.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(inter_dense.session_id.nunique()))\n",
    "\n",
    "store_path = \"../../data/cb12/\"\n",
    "inter_dense.to_csv(store_path + \"filtered.csv\", sep='\\t')\n",
    "\n",
    "print('Partitioning data')\n",
    "# last-session-out partitioning\n",
    "train_full_sessions, test_sessions = last_n_days_out_split(inter_dense, n=14,\n",
    "                                                            user_key='user_id',\n",
    "                                                            item_key='item_id',\n",
    "                                                            session_key='session_id',\n",
    "                                                            time_key='created_at',\n",
    "                                                            clean_test=True)\n",
    "train_valid_sessions, valid_sessions = last_n_days_out_split(train_full_sessions, n=14,\n",
    "                                                              user_key='user_id',\n",
    "                                                              item_key='item_id',\n",
    "                                                              session_key='session_id',\n",
    "                                                              time_key='created_at',\n",
    "                                                              clean_test=True)\n",
    "\n",
    "print(\"Data - Sessions: \" + str(len(inter_dense.session_id.unique())))\n",
    "print(\"Training - Sessions: \" + str(len(train_full_sessions.session_id.unique())))\n",
    "print(\"Testing - Sessions: \" + str(len(test_sessions.session_id.unique())))\n",
    "print(\"Train Validating - Sessions: \" + str(len(train_valid_sessions.session_id.unique())))\n",
    "print(\"Test Validating - Sessions: \" + str(len(valid_sessions.session_id.unique())))\n",
    "\n",
    "train_full_sessions.to_csv(store_path + \"train_d14.csv\", sep='\\t')\n",
    "test_sessions.to_csv(store_path + \"test_d14.csv\", sep='\\t')\n",
    "train_valid_sessions.to_csv(store_path + \"valid_train_d14.csv\", sep='\\t')\n",
    "valid_sessions.to_csv(store_path + \"valid_test_d14.csv\", sep='\\t')\n",
    "\n",
    "print('Train Num items: {}'.format(train_full_sessions.item_id.nunique()))\n",
    "print('Train Num sessions: {}'.format(train_full_sessions.session_id.nunique()))\n",
    "print('Train Num events: {}'.format(len(train_full_sessions)))\n",
    "print('Test Num items: {}'.format(test_sessions.item_id.nunique()))\n",
    "print('Test Num sessions: {}'.format(test_sessions.session_id.nunique()))\n",
    "print('Test Num events: {}'.format(len(test_sessions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
