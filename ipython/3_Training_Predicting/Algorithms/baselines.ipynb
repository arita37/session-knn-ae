{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting baselines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile baselines.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jun 26 11:57:27 2015\n",
    "\n",
    "@author: Bal√°zs Hidasi\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class RandomPred:\n",
    "    '''\n",
    "    RandomPred()\n",
    "    \n",
    "    Initializes a random predcitor, which is a baseline predictor that gives back a random score for each item.  \n",
    "    \n",
    "    '''\n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Dummy function for training.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "        return pd.Series(data=np.random.rand(len(predict_for_item_ids)), index=predict_for_item_ids)\n",
    "\n",
    "class Pop:\n",
    "    '''\n",
    "    Pop(top_n=100, item_key='ItemId', support_by_key=None)\n",
    "    \n",
    "    Popularity predictor that gives higher scores to items with larger support.\n",
    "    \n",
    "    The score is given by:\n",
    "    \n",
    "    .. math::\n",
    "        r_{i}=\\\\frac{supp_i}{(1+supp_i)}\n",
    "        \n",
    "    Parameters\n",
    "    --------\n",
    "    top_n : int\n",
    "        Only give back non-zero scores to the top N ranking items. Should be higher or equal than the cut-off of your evaluation. (Default value: 100)\n",
    "    item_key : string\n",
    "        The header of the item IDs in the training data. (Default value: 'ItemId')\n",
    "    support_by_key : string or None\n",
    "        If not None, count the number of unique values of the attribute of the training data given by the specified header. If None, count the events. (Default value: None)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, top_n = 100, item_key = 'ItemId', support_by_key = None):\n",
    "        self.top_n = top_n\n",
    "        self.item_key = item_key\n",
    "        self.support_by_key = support_by_key\n",
    "    \n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "        grp = data.groupby(self.item_key)\n",
    "        self.pop_list = grp.size() if self.support_by_key is None else grp[self.support_by_key].nunique()\n",
    "        self.pop_list = self.pop_list / (self.pop_list + 1)\n",
    "        self.pop_list.sort_values(ascending=False, inplace=True)\n",
    "        self.pop_list = self.pop_list.head(self.top_n)  \n",
    "    \n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "        preds = np.zeros(len(predict_for_item_ids))\n",
    "        mask = np.in1d(predict_for_item_ids, self.pop_list.index)\n",
    "        preds[mask] = self.pop_list[predict_for_item_ids[mask]]\n",
    "        return pd.Series(data=preds, index=predict_for_item_ids)\n",
    "        \n",
    "class SessionPop:\n",
    "    '''\n",
    "    SessionPop(top_n=100, item_key='ItemId', support_by_key=None)\n",
    "    \n",
    "    Session popularity predictor that gives higher scores to items with higher number of occurrences in the session. Ties are broken up by adding the popularity score of the item.\n",
    "    \n",
    "    The score is given by:\n",
    "    \n",
    "    .. math::\n",
    "        r_{s,i} = supp_{s,i} + \\\\frac{supp_i}{(1+supp_i)}\n",
    "        \n",
    "    Parameters\n",
    "    --------\n",
    "    top_n : int\n",
    "        Only give back non-zero scores to the top N ranking items. Should be higher or equal than the cut-off of your evaluation. (Default value: 100)\n",
    "    item_key : string\n",
    "        The header of the item IDs in the training data. (Default value: 'ItemId')\n",
    "    support_by_key : string or None\n",
    "        If not None, count the number of unique values of the attribute of the training data given by the specified header. If None, count the events. (Default value: None)\n",
    "    \n",
    "    '''    \n",
    "    def __init__(self, top_n = 100, item_key = 'ItemId', support_by_key = None):\n",
    "        self.top_n = top_n\n",
    "        self.item_key = item_key\n",
    "        self.support_by_key = support_by_key\n",
    "    \n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "        grp = data.groupby(self.item_key)\n",
    "        self.pop_list = grp.size() if self.support_by_key is None else grp[self.support_by_key].nunique()\n",
    "        self.pop_list = self.pop_list / (self.pop_list + 1)\n",
    "        self.pop_list.sort_values(ascending=False, inplace=True)\n",
    "        self.pop_list = self.pop_list.head(self.top_n)\n",
    "        self.prev_session_id = -1\n",
    "         \n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event. If changed during subsequent calls, a new session starts.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "        if self.prev_session_id != session_id:\n",
    "            self.prev_session_id = session_id\n",
    "            self.pers = dict()\n",
    "        v = self.pers.get(input_item_id)\n",
    "        if v:\n",
    "            self.pers[input_item_id] = v + 1\n",
    "        else:\n",
    "            self.pers[input_item_id] = 1\n",
    "        preds = np.zeros(len(predict_for_item_ids))\n",
    "        mask = np.in1d(predict_for_item_ids, self.pop_list.index)\n",
    "        ser = pd.Series(self.pers)\n",
    "        preds[mask] = self.pop_list[predict_for_item_ids[mask]] \n",
    "        mask = np.in1d(predict_for_item_ids, ser.index)\n",
    "        preds[mask] += ser[predict_for_item_ids[mask]]\n",
    "        return pd.Series(data=preds, index=predict_for_item_ids)\n",
    " \n",
    "class ItemKNN:\n",
    "    '''\n",
    "    ItemKNN(n_sims = 100, lmbd = 20, alpha = 0.5, session_key = 'SessionId', item_key = 'ItemId', time_key = 'Time')\n",
    "    \n",
    "    Item-to-item predictor that computes the the similarity to all items to the given item.\n",
    "    \n",
    "    Similarity of two items is given by:\n",
    "    \n",
    "    .. math::\n",
    "        s_{i,j}=\\sum_{s}I\\{(s,i)\\in D & (s,j)\\in D\\} / (supp_i+\\\\lambda)^{\\\\alpha}(supp_j+\\\\lambda)^{1-\\\\alpha}\n",
    "        \n",
    "    Parameters\n",
    "    --------\n",
    "    n_sims : int\n",
    "        Only give back non-zero scores to the N most similar items. Should be higher or equal than the cut-off of your evaluation. (Default value: 100)\n",
    "    lmbd : float\n",
    "        Regularization. Discounts the similarity of rare items (incidental co-occurrences). (Default value: 20)\n",
    "    alpha : float\n",
    "        Balance between normalizing with the supports of the two items. 0.5 gives cosine similarity, 1.0 gives confidence (as in association rules).\n",
    "    session_key : string\n",
    "        header of the session ID column in the input file (default: 'SessionId')\n",
    "    item_key : string\n",
    "        header of the item ID column in the input file (default: 'ItemId')\n",
    "    time_key : string\n",
    "        header of the timestamp column in the input file (default: 'Time')\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    def __init__(self, n_sims = 100, lmbd = 20, alpha = 0.5, session_key = 'SessionId', item_key = 'ItemId', time_key = 'Time'):\n",
    "        self.n_sims = n_sims\n",
    "        self.lmbd = lmbd\n",
    "        self.alpha = alpha\n",
    "        self.item_key = item_key\n",
    "        self.session_key = session_key\n",
    "        self.time_key = time_key\n",
    "\n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "        data.set_index(np.arange(len(data)), inplace=True)\n",
    "        itemids = data[self.item_key].unique()\n",
    "        n_items = len(itemids) \n",
    "        data = pd.merge(data, pd.DataFrame({self.item_key:itemids, 'ItemIdx':np.arange(len(itemids))}), on=self.item_key, how='inner')\n",
    "        sessionids = data[self.session_key].unique()\n",
    "        data = pd.merge(data, pd.DataFrame({self.session_key:sessionids, 'SessionIdx':np.arange(len(sessionids))}), on=self.session_key, how='inner')\n",
    "        supp = data.groupby('SessionIdx').size()\n",
    "        session_offsets = np.zeros(len(supp)+1, dtype=np.int32)\n",
    "        session_offsets[1:] = supp.cumsum()\n",
    "        index_by_sessions = data.sort_values(['SessionIdx', self.time_key]).index.values\n",
    "        supp = data.groupby('ItemIdx').size()\n",
    "        item_offsets = np.zeros(n_items+1, dtype=np.int32)\n",
    "        item_offsets[1:] = supp.cumsum()\n",
    "        index_by_items = data.sort_values(['ItemIdx', self.time_key]).index.values\n",
    "        self.sims = dict()\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        for i in range(n_items):\n",
    "            if count % 1000 == 0:\n",
    "                print( 'Train itemKNN process: ', count, ' of ', n_items, ' items: ', ( count / n_items * 100.0 ))\n",
    "        \n",
    "            count += 1\n",
    "            \n",
    "            iarray = np.zeros(n_items)\n",
    "            start = item_offsets[i]\n",
    "            end = item_offsets[i+1]\n",
    "            for e in index_by_items[start:end]:\n",
    "                uidx = data.SessionIdx.values[e]\n",
    "                ustart = session_offsets[uidx]\n",
    "                uend = session_offsets[uidx+1]\n",
    "                user_events = index_by_sessions[ustart:uend]\n",
    "                iarray[data.ItemIdx.values[user_events]] += 1\n",
    "            iarray[i] = 0\n",
    "            norm = np.power((supp[i] + self.lmbd), self.alpha) * np.power((supp.values + self.lmbd), (1.0 - self.alpha))\n",
    "            norm[norm == 0] = 1\n",
    "            iarray = iarray / norm\n",
    "            indices = np.argsort(iarray)[-1:-1-self.n_sims:-1]\n",
    "            self.sims[itemids[i]] = pd.Series(data=iarray[indices], index=itemids[indices])\n",
    "    \n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''\n",
    "        preds = np.zeros(len(predict_for_item_ids))\n",
    "        sim_list = self.sims[input_item_id]\n",
    "        mask = np.in1d(predict_for_item_ids, sim_list.index)\n",
    "        preds[mask] = sim_list[predict_for_item_ids[mask]]\n",
    "        return pd.Series(data=preds, index=predict_for_item_ids)\n",
    "\n",
    "class BPR:\n",
    "    '''\n",
    "    BPR(n_factors = 100, n_iterations = 10, learning_rate = 0.01, lambda_session = 0.0, lambda_item = 0.0, sigma = 0.05, init_normal = False, session_key = 'SessionId', item_key = 'ItemId')\n",
    "    \n",
    "    Bayesian Personalized Ranking Matrix Factorization (BPR-MF). During prediction time, the current state of the session is modelled as the average of the feature vectors of the items that have occurred in it so far.\n",
    "        \n",
    "    Parameters\n",
    "    --------\n",
    "    n_factor : int\n",
    "        The number of features in a feature vector. (Default value: 100)\n",
    "    n_iterations : int\n",
    "        The number of epoch for training. (Default value: 10)\n",
    "    learning_rate : float\n",
    "        Learning rate. (Default value: 0.01)\n",
    "    lambda_session : float\n",
    "        Regularization for session features. (Default value: 0.0)\n",
    "    lambda_item : float\n",
    "        Regularization for item features. (Default value: 0.0)\n",
    "    sigma : float\n",
    "        The width of the initialization. (Default value: 0.05)\n",
    "    init_normal : boolean\n",
    "        Whether to use uniform or normal distribution based initialization.\n",
    "    session_key : string\n",
    "        header of the session ID column in the input file (default: 'SessionId')\n",
    "    item_key : string\n",
    "        header of the item ID column in the input file (default: 'ItemId')\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, n_factors = 100, n_iterations = 10, learning_rate = 0.01, lambda_session = 0.0, lambda_item = 0.0, sigma = 0.05, init_normal = False, session_key = 'SessionId', item_key = 'ItemId'):\n",
    "        self.n_factors = n_factors\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_session = lambda_session\n",
    "        self.lambda_item = lambda_item\n",
    "        self.sigma = sigma\n",
    "        self.init_normal = init_normal\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.current_session = None\n",
    "\n",
    "    def init(self, data):\n",
    "        self.U = np.random.rand(self.n_sessions, self.n_factors) * 2 * self.sigma - self.sigma if not self.init_normal else np.random.randn(self.n_sessions, self.n_factors) * self.sigma\n",
    "        self.I = np.random.rand(self.n_items, self.n_factors) * 2 * self.sigma - self.sigma if not self.init_normal else np.random.randn(self.n_items, self.n_factors) * self.sigma\n",
    "        self.bU = np.zeros(self.n_sessions)\n",
    "        self.bI = np.zeros(self.n_items)\n",
    "    \n",
    "    def update(self, uidx, p, n):\n",
    "        uF = np.copy(self.U[uidx,:])\n",
    "        iF1 = np.copy(self.I[p,:])\n",
    "        iF2 = np.copy(self.I[n,:])\n",
    "        sigm = self.sigmoid(iF1.T.dot(uF) - iF2.T.dot(uF) + self.bI[p] - self.bI[n])\n",
    "        c = 1.0 - sigm\n",
    "        self.U[uidx,:] += self.learning_rate * (c * (iF1 - iF2) - self.lambda_session * uF)\n",
    "        self.I[p,:] += self.learning_rate * (c * uF - self.lambda_item * iF1)\n",
    "        self.I[n,:] += self.learning_rate * (-c * uF - self.lambda_item * iF2)\n",
    "        return np.log(sigm)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Trains the predictor.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        data: pandas.DataFrame\n",
    "            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n",
    "            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n",
    "            \n",
    "        '''\n",
    "        itemids = data[self.item_key].unique()\n",
    "        self.n_items = len(itemids)\n",
    "        self.itemidmap = pd.Series(data=np.arange(self.n_items), index=itemids)\n",
    "        sessionids = data[self.session_key].unique()\n",
    "        self.n_sessions = len(sessionids)\n",
    "        data = pd.merge(data, pd.DataFrame({self.item_key:itemids, 'ItemIdx':np.arange(self.n_items)}), on=self.item_key, how='inner')\n",
    "        data = pd.merge(data, pd.DataFrame({self.session_key:sessionids, 'SessionIdx':np.arange(self.n_sessions)}), on=self.session_key, how='inner')     \n",
    "        self.init(data)\n",
    "        for it in range(self.n_iterations):\n",
    "            c = []\n",
    "            for e in np.random.permutation(len(data)):\n",
    "                uidx = data.SessionIdx.values[e]\n",
    "                iidx = data.ItemIdx.values[e]\n",
    "                iidx2 = data.ItemIdx.values[np.random.randint(self.n_items)]\n",
    "                err = self.update(uidx, iidx, iidx2)\n",
    "                c.append(err)\n",
    "            print(it, np.mean(c))\n",
    "    \n",
    "    def predict_next(self, session_id, input_item_id, predict_for_item_ids):\n",
    "        '''\n",
    "        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n",
    "                \n",
    "        Parameters\n",
    "        --------\n",
    "        session_id : int or string\n",
    "            The session IDs of the event.\n",
    "        input_item_id : int or string\n",
    "            The item ID of the event. Must be in the set of item IDs of the training set.\n",
    "        predict_for_item_ids : 1D array\n",
    "            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        out : pandas.Series\n",
    "            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n",
    "        \n",
    "        '''      \n",
    "        iidx = self.itemidmap[input_item_id]\n",
    "        if self.current_session is None or self.current_session != session_id:\n",
    "            self.current_session = session_id\n",
    "            self.session = [iidx]\n",
    "        else:\n",
    "            self.session.append(iidx)\n",
    "        uF = self.I[self.session].mean(axis=0)\n",
    "        iIdxs = self.itemidmap[predict_for_item_ids]\n",
    "        return pd.Series(data=self.I[iIdxs].dot(uF) + self.bI[iIdxs], index=predict_for_item_ids)\n",
    "             \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
