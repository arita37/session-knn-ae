{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import pickle\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"recsys17/\"\n",
    "path =  \"../../data/\"\n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\"\n",
    "model_path = \"models/\"\n",
    "model_path_valid = \"models/valid/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOP1(y_true, y_pred):\n",
    "    y1 = y_pred * y_true\n",
    "    y2 = K.sum(y1, axis=1)[:, np.newaxis]\n",
    "    y3 = y_true - y1\n",
    "    return (K.sum(K.sigmoid(y_pred - y2)) + y3 * y3) / tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "\n",
    "loss = TOP1\n",
    "\n",
    "def create_prnn_model(left_input_size, right_input_size, batch_size = 512, hidden_units = 100, o_activation='softmax', lr = 0.001):   \n",
    "    emb_size = 50\n",
    "    size = emb_size\n",
    "\n",
    "    # left input - item vector\n",
    "    input_left = Input(batch_shape=(batch_size, 1, left_input_size), name='input_left')\n",
    "    gru_left, gru_left_states = GRU(hidden_units, stateful=True, return_state=True, name='gru_left')(input_left)\n",
    "\n",
    "    # right input - feature vector\n",
    "    input_right = Input(batch_shape=(batch_size, 1, right_input_size), name='input_right')\n",
    "    gru_right, gru_right_states = GRU(hidden_units, stateful=True, return_state=True, name='gru_right')(input_right)\n",
    "    \n",
    "    # merging both layers and creating the model\n",
    "    merged = concatenate([gru_left, gru_right])\n",
    "    #change softmax per another activation funciton?\n",
    "    output = Dense(left_input_size, activation=o_activation, name='output')(merged)\n",
    "    model = Model(inputs=[input_left, input_right], outputs=output, name='gru4rec')\n",
    "    \n",
    "    encoder = Model(inputs=[input_left, input_right], outputs=merged)\n",
    "\n",
    "    # define model's optimizer\n",
    "    #optimizer = optim.Optimizer(optimizer=self.optimizer, lr=self.lr)\n",
    "    #opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    opt = keras.optimizers.Adagrad(lr=lr)\n",
    "    \n",
    "    # define model's loss function --> implement here the top1 loss function\n",
    "#     loss_function = loss.LossFunction(loss_type=self.loss_function)\n",
    "    #model.compile(loss=loss_function, optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "    filepath = model_path_valid + 'prnn_studo_checkpoint.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "    callbacks_list = []\n",
    "    model.summary()\n",
    "    #plot_model(model, show_shapes=True, to_file='rnn-structure.png')\n",
    "    return model, encoder\n",
    "\n",
    "def get_states(model):\n",
    "    #return the actual states of the layers\n",
    "    return [K.get_value(s) for s,_ in model.state_updates]\n",
    "\n",
    "\n",
    "def freeze_layer(model, layer_name, lr):\n",
    "    if layer_name == 'gru_left':\n",
    "        # gru left layer will not be trained this mini batch\n",
    "        model.get_layer(layer_name).trainable = False\n",
    "        # but gru right will\n",
    "        model.get_layer('gru_right').trainable = True\n",
    "    elif layer_name == 'gru_right':\n",
    "        # gru right layer will not be trained this mini batch\n",
    "        model.get_layer(layer_name).trainable = False\n",
    "        # but gru left will\n",
    "        model.get_layer('gru_left').trainable = True\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    opt = keras.optimizers.Adagrad(lr=lr)\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../data/' + dataset + 'processed/valid_train_14d.csv'\n",
    "train = pd.read_csv(train_path, sep='\\t')[['session_id', 'item_id', 'created_at']]\n",
    "\n",
    "interactions = pd.read_csv('../../data/' + dataset + 'raw/interactions.csv', header=0, sep='\\t')\n",
    "items = pd.read_csv('../../data/' + dataset + 'raw/items.csv', header=0, sep='\\t')\n",
    "view_fields = [\"item_id\", \"career_level\", \"discipline_id\", \"industry_id\", \"country\", \"is_payed\", \"region\", \"employment\"]\n",
    "common_items = items.merge(interactions, on=['item_id'])[view_fields].drop_duplicates()\n",
    "\n",
    "## to test - delete after\n",
    "#train = train.head(6)\n",
    "#print(train.session_id)\n",
    "#common_items = common_items.merge(train, on=['item_id'])[view_fields].drop_duplicates()\n",
    "\n",
    "## delete \n",
    "\n",
    "\n",
    "item_count = len(train['item_id'].unique())\n",
    "session_count = len(train['created_at'].unique())\n",
    "print(len(common_items))\n",
    "# common_items.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_items.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studo items need to be converted to dummies\n",
    "\n",
    "common = common_items\n",
    "\n",
    "common[\"country\"] = common[\"country\"].astype('str')\n",
    "common[\"career_level\"] = common[\"career_level\"].astype('str')\n",
    "common[\"industry_id\"] = common[\"industry_id\"].astype('str')\n",
    "common[\"is_payed\"] = common[\"is_payed\"].astype('str')\n",
    "common[\"region\"] = common[\"region\"].astype('str')\n",
    "common[\"employment\"] = common[\"employment\"].astype('str')\n",
    "common[\"discipline_id\"] = common[\"discipline_id\"].astype('str')\n",
    "\n",
    "df2 = pd.DataFrame(index=common.index)\n",
    "s1 = pd.get_dummies(common[\"country\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"country\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "s1 = pd.get_dummies(common[\"career_level\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"career_level\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "df2 = df2.drop([\"country_\", \"career_level_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "s1 = pd.get_dummies(common[\"industry_id\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"industry_id\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "s1 = pd.get_dummies(common[\"is_payed\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"is_payed\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "df2 = df2.drop([\"industry_id_\", \"is_payed_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "\n",
    "s1 = pd.get_dummies(common[\"region\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"region\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "s1 = pd.get_dummies(common[\"employment\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"employment\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "df2 = df2.drop([\"region_\", \"employment_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "s1 = pd.get_dummies(common[\"discipline_id\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"discipline_id\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "df2 = df2.drop([\"discipline_id_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "\n",
    "common = common.drop([\"country\", \"career_level\", \"industry_id\", \"is_payed\", \"region\", \"employment\", \"discipline_id\"], axis=1)\n",
    "df2 = pd.concat([common, df2], axis=1)\n",
    "\n",
    "one_hot = df2\n",
    "print(one_hot.shape)\n",
    "# number of content features per item\n",
    "feature_size = one_hot.shape[1] - 1\n",
    "\n",
    "item_encodings = {}\n",
    "for index, row in one_hot.iterrows():\n",
    "    item_id = row[\"item_id\"]\n",
    "    item_encodings[item_id] = row.values[1:]\n",
    "\n",
    "print(len(item_encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_size)\n",
    "print(item_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, data, sep='\\t', session_key='session_id', item_key='item_id', time_key='created_at', n_samples=-1, itemmap=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "\n",
    "        # Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets() \n",
    "        #array of the positions where there is a change of session. \n",
    "        #len = len(session_idx_arr) + 1\n",
    "        \n",
    "        self.session_idx_arr = self.order_session_idx() \n",
    "        #array of sessions [0 1 2 3 4 .... n-1]\n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.item_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.dataset.df\n",
    "        \n",
    "        session_key='session_id'\n",
    "        item_key='item_id'\n",
    "        time_key='created_at'\n",
    "        self.n_items = df[item_key].nunique()\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        #print(click_offsets)\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "        #print(session_idx_arr)\n",
    "        \n",
    "        iters = np.arange(self.batch_size)\n",
    "        #iters = np.arange(1)\n",
    "\n",
    "        maxiter = iters.max()\n",
    "                \n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        #print(start)\n",
    "        #print(end)\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            #minimum lenght of all the sessions\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                inp = idx_input\n",
    "                target = idx_target\n",
    "                yield inp, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prnn(model, lr, loader, layer_freezing_enabled = False, num_epochs = 10):\n",
    "    for epoch in range(0, num_epochs):\n",
    "        print(\"Epoch: \" + str(epoch+1))\n",
    "        epoch_loss = 0  \n",
    "\n",
    "        i = 0\n",
    "        for feat, target, mask in loader:\n",
    "            #feat = np array size BATCH_SIZE with the item indexes of the first items of the first BATCH_SIZE sessions\n",
    "            #comvert feat to an array size (BATCH_SIZE, 26723) of one hot encoding the indes with loader.n_items\n",
    "\n",
    "            input_oh = to_categorical(feat, num_classes=loader.n_items)\n",
    "            #convert from shape (BATCH_SIZE, 26723) to (BATCH_SIZE, 1, 26723)\n",
    "            input_oh = np.expand_dims(input_oh, axis=1)        \n",
    "\n",
    "            # with the argmax function you get back again the feat/target np array (arg_input = feat)\n",
    "            ### arg_input = np.argmax(to_categorical(feat, num_classes=loader.n_items), axis=1)\n",
    "            ### arg_output = np.argmax(to_categorical(target, num_classes=loader.n_items), axis=1)\n",
    "            input_feature = np.array([])\n",
    "\n",
    "            for line in feat:\n",
    "                #result = int(mapitem[(mapitem.item_idx == line)].item_id.values)\n",
    "                result = mapitem[(mapitem.item_idx == line)].item_id.values[0]\n",
    "                #print(result)\n",
    "                feature_vector = item_encodings[result]\n",
    "                input_feature = np.append(input_feature, feature_vector)\n",
    "\n",
    "            input_feature = input_feature.reshape(batch_size, 1, feature_size)\n",
    "\n",
    "            #target = np array size BATCH_SIZE with the item indexes of the TARGET items of the feat array items\n",
    "            target_oh = to_categorical(target, num_classes=loader.n_items)\n",
    "\n",
    "            #calculate the loss between the input and the expected output\n",
    "\n",
    "            if layer_freezing_enabled:\n",
    "                if i % 2 is 0:\n",
    "                    model = freeze_layer(model, 'gru_left', lr = lr)\n",
    "                else:\n",
    "                    model = freeze_layer(model, 'gru_right', lr = lr)\n",
    "\n",
    "            tr_loss = model.train_on_batch([input_oh, input_feature], target_oh)\n",
    "            epoch_loss += tr_loss[0]\n",
    "\n",
    "            i = i + 1\n",
    "        print(\"Epoch loss: \" + str(epoch_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "acts = ['softmax', 'tanh']\n",
    "l_sizes = [100, 1000]\n",
    "lrs = [0.001, 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in acts:\n",
    "    for ls in l_sizes:\n",
    "        for lr in lrs:\n",
    "            train_dataset = SessionDataset(train)\n",
    "            loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "            mapitem = loader.dataset.itemmap\n",
    "            # define model\n",
    "            model, encoder = create_prnn_model(item_count, feature_size, batch_size=batch_size, hidden_units = ls, o_activation = act, lr = lr)\n",
    "            # train model\n",
    "            model = train_prnn(model, lr, loader)\n",
    "            model_name = \"recsys17_prnn_a_\" + act + \"_ls_\" + str(ls) + \"_lr_\" + str(lr) + \".model\"\n",
    "            pickle.dump(model, open(model_path_valid + model_name, 'wb'), protocol=4)\n",
    "            print(\"Stored model in: \" + model_path_valid + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "keras.losses.TOP1 = TOP1\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "\n",
    "train_dataset = SessionDataset(train)\n",
    "loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "    \n",
    "def predict_function(sid, test_session, pr, item_idx_map, idx_item_map, cut_off=20, \n",
    "                     session_key='session_id', item_key='item_id', time_key='created_at'):\n",
    "    test_session.sort_values([time_key], inplace=True)\n",
    "    # get first and only session_id (as we grouped it before calling this method)\n",
    "    session_id = test_session[session_key].unique()[0]\n",
    "\n",
    "    log_columns = [\"session_id\", \"input_items\", \"input_count\", \"position\", \"remaining_items\", \"remaining_count\", \"predictions\"]\n",
    "    log_df = pd.DataFrame(columns = log_columns)\n",
    "\n",
    "    session_length = len(test_session)\n",
    "    il = a = np.zeros((batch_size, 1, len(item_idx_map)))\n",
    "    ir = a = np.zeros((batch_size, 1, 79))\n",
    "    \n",
    "    for i in range(session_length -1):\n",
    "        # use current item as reference point (rest is for testing)\n",
    "        current_item_id = test_session[item_key].values[i]\n",
    "\n",
    "        item_vec = np.zeros(len(item_idx_map), dtype=int)\n",
    "        item_idx = item_idx_map[current_item_id]\n",
    "        item_vec[item_idx] = 1\n",
    "        # set vector in batch input\n",
    "        il[i, 0] = item_vec\n",
    "        \n",
    "        item_features = item_encodings[current_item_id]\n",
    "        #item_features = item_features.reshape(1,1, len(item_features))\n",
    "        ir[i, 0] = item_features\n",
    "        \n",
    "    # do batch prediction\n",
    "    pred = model.predict([il, ir], batch_size=batch_size)\n",
    "    \n",
    "    # for every subsession prediction\n",
    "    for i in range(session_length-1):\n",
    "        preds = pred[i]\n",
    "        topn_idx_preds = preds.argsort()[-cut_off:][::-1]\n",
    "        \n",
    "        predictions = []\n",
    "        # for every recommended item index\n",
    "        for item_idx in topn_idx_preds:\n",
    "            pred_item = idx_item_map[item_idx]\n",
    "            predictions.append(pred_item)\n",
    "            \n",
    "        current_input_set = test_session[item_key].values[:i+1]\n",
    "        remaining_test_set = test_session[item_key].values[i+1:]\n",
    "        \n",
    "        position = \"MID\"\n",
    "        if i == 0:\n",
    "            position = \"FIRST\"\n",
    "        if len(remaining_test_set) == 1:\n",
    "            position = \"LAST\"\n",
    "        \n",
    "        log_df = log_df.append({\n",
    "            \"session_id\": sid,\n",
    "            \"input_items\":  ','.join(map(str, current_input_set)),\n",
    "            \"input_count\":  len(current_input_set),\n",
    "            \"position\": position,\n",
    "            \"remaining_items\":  ','.join(map(str, remaining_test_set)),\n",
    "            \"remaining_count\":  len(remaining_test_set),\n",
    "            \"predictions\": ','.join(map(str, predictions))\n",
    "        }, ignore_index=True) \n",
    "            \n",
    "    \n",
    "    log_df['input_count'] = log_df['input_count'].astype(int)\n",
    "    log_df['remaining_count'] = log_df['remaining_count'].astype(int)\n",
    "    \n",
    "    return log_df\n",
    "\n",
    "test_path = '../../data/' + dataset + 'processed/valid_test_14d.csv'\n",
    "test = pd.read_csv(test_path, sep='\\t')[['session_id', 'item_id', 'created_at']]\n",
    "test_dataset = SessionDataset(test)\n",
    "test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "session_groups = test.groupby(\"session_id\")\n",
    "mapitem = loader.dataset.itemmap\n",
    "\n",
    "item_idx_map = {}\n",
    "idx_item_map = {}\n",
    "for index, row in mapitem.iterrows():\n",
    "    item_id = row[\"item_id\"]\n",
    "    item_idx = row[\"item_idx\"]\n",
    "    item_idx_map[item_id] = item_idx\n",
    "    idx_item_map[item_idx] = item_id\n",
    "\n",
    "    \n",
    "predict_path = \"../../data/recsys17/interim/predict/hyperparam/\"\n",
    "\n",
    "for act in acts:\n",
    "    for ls in l_sizes:\n",
    "        for lr in lrs:\n",
    "            model_name = \"recsys17_prnn_a_\" + act + \"_ls_\" + str(ls) + \"_lr_\" + str(lr) + \".model\"\n",
    "            model = pickle.load(open(model_path_valid + model_name, 'rb'))\n",
    "            \n",
    "            res_list = []\n",
    "            # predict\n",
    "            report_freq = len(session_groups) // 5 \n",
    "            count = 0\n",
    "            for sid, session in session_groups:\n",
    "                pred_df = predict_function(sid, session, model, item_idx_map, idx_item_map)\n",
    "                res_list.append(pred_df)\n",
    "                # reset states\n",
    "                model.get_layer('gru_left').reset_states()\n",
    "                model.get_layer('gru_right').reset_states()\n",
    "                # print progress\n",
    "                count += 1\n",
    "                if count % report_freq == 0:\n",
    "                    print(\"Predicted for \" + str(count) + \" sessions. \" + str(len(session_groups) - count) + \" sessions to go.\" )\n",
    "            # concat results\n",
    "            res = pd.concat(res_list)\n",
    "            res = res.reindex(columns = [\"session_id\", \"input_items\", \"input_count\", \"position\", \"remaining_items\", \"remaining_count\", \"predictions\"])\n",
    "\n",
    "            store_name = model_name.replace(\"recsys17_\", \"\").replace(\".model\", \"\")\n",
    "            res.to_csv(predict_path + \"test_14d_\" + store_name + \".csv\", sep='\\t')\n",
    "            \n",
    "            print(\"Stored predictions: \" + predict_path + \"test_14d_\" + store_name + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set data for final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data\n",
    "\n",
    "train_path = '../../data/' + dataset + 'processed/train_14d.csv'\n",
    "train = pd.read_csv(train_path, sep='\\t')[['session_id', 'item_id', 'created_at']]\n",
    "\n",
    "interactions = pd.read_csv('../../data/' + dataset + 'raw/interactions.csv', header=0, sep='\\t')\n",
    "items = pd.read_csv('../../data/' + dataset + 'raw/items.csv', header=0, sep='\\t')\n",
    "view_fields = [\"item_id\", \"career_level\", \"discipline_id\", \"industry_id\", \"country\", \"is_payed\", \"region\", \"employment\"]\n",
    "common_items = items.merge(interactions, on=['item_id'])[view_fields].drop_duplicates()\n",
    "\n",
    "item_count = len(train['item_id'].unique())\n",
    "print(item_count)\n",
    "session_count = len(train['created_at'].unique())\n",
    "print(len(common_items))\n",
    "\n",
    "# Studo items need to be converted to dummies\n",
    "\n",
    "common = common_items\n",
    "\n",
    "common[\"country\"] = common[\"country\"].astype('str')\n",
    "common[\"career_level\"] = common[\"career_level\"].astype('str')\n",
    "common[\"industry_id\"] = common[\"industry_id\"].astype('str')\n",
    "common[\"is_payed\"] = common[\"is_payed\"].astype('str')\n",
    "common[\"region\"] = common[\"region\"].astype('str')\n",
    "common[\"employment\"] = common[\"employment\"].astype('str')\n",
    "common[\"discipline_id\"] = common[\"discipline_id\"].astype('str')\n",
    "\n",
    "df2 = pd.DataFrame(index=common.index)\n",
    "s1 = pd.get_dummies(common[\"country\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"country\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "s1 = pd.get_dummies(common[\"career_level\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"career_level\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "df2 = df2.drop([\"country_\", \"career_level_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "s1 = pd.get_dummies(common[\"industry_id\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"industry_id\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "s1 = pd.get_dummies(common[\"is_payed\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"is_payed\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "df2 = df2.drop([\"industry_id_\", \"is_payed_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "\n",
    "s1 = pd.get_dummies(common[\"region\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"region\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "s1 = pd.get_dummies(common[\"employment\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"employment\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "df2 = df2.drop([\"region_\", \"employment_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "s1 = pd.get_dummies(common[\"discipline_id\"].fillna(\"\").str.split(\",\").apply(pd.Series).stack(), prefix=\"discipline_id\").sum(level=0)\n",
    "df2 = pd.concat([df2, s1], axis=1)\n",
    "\n",
    "df2 = df2.drop([\"discipline_id_\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "\n",
    "common = common.drop([\"country\", \"career_level\", \"industry_id\", \"is_payed\", \"region\", \"employment\", \"discipline_id\"], axis=1)\n",
    "df2 = pd.concat([common, df2], axis=1)\n",
    "\n",
    "one_hot = df2\n",
    "print(one_hot.shape)\n",
    "# number of content features per item\n",
    "feature_size = one_hot.shape[1] - 1\n",
    "\n",
    "item_encodings = {}\n",
    "for index, row in one_hot.iterrows():\n",
    "    item_id = row[\"item_id\"]\n",
    "    item_encodings[item_id] = row.values[1:]\n",
    "\n",
    "print(len(item_encodings))\n",
    "\n",
    "# load data\n",
    "\n",
    "train_dataset = SessionDataset(train)\n",
    "loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "mapitem = loader.dataset.itemmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best params\n",
    "ls = 100\n",
    "act = \"tanh\"\n",
    "lr = 0.01\n",
    "# define model\n",
    "model, encoder = create_prnn_model(item_count, feature_size, batch_size=batch_size, hidden_units = ls, o_activation = act, lr = lr)\n",
    "# train model\n",
    "model = train_prnn(model, lr, loader)\n",
    "model_name = \"recsys17_prnn_a_\" + act + \"_ls_\" + str(ls) + \"_lr_\" + str(lr) + \".model\"\n",
    "pickle.dump(model, open(model_path + model_name, 'wb'), protocol=4)\n",
    "print(\"Stored model in: \" + model_path + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "keras.losses.TOP1 = TOP1\n",
    "\n",
    "train_dataset = SessionDataset(train)\n",
    "loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "    \n",
    "\n",
    "test_path = '../../data/' + dataset + 'processed/test_14d.csv'\n",
    "test = pd.read_csv(test_path, sep='\\t')[['session_id', 'item_id', 'created_at']]\n",
    "test_dataset = SessionDataset(test)\n",
    "test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "session_groups = test.groupby(\"session_id\")\n",
    "mapitem = loader.dataset.itemmap\n",
    "\n",
    "item_idx_map = {}\n",
    "idx_item_map = {}\n",
    "for index, row in mapitem.iterrows():\n",
    "    item_id = row[\"item_id\"]\n",
    "    item_idx = row[\"item_idx\"]\n",
    "    item_idx_map[item_id] = item_idx\n",
    "    idx_item_map[item_idx] = item_id\n",
    "\n",
    "    \n",
    "predict_path = \"../../data/recsys17/interim/predict/base/\"\n",
    "\n",
    "\n",
    "model_name = \"recsys17_prnn_a_\" + act + \"_ls_\" + str(ls) + \"_lr_\" + str(lr) + \".model\"\n",
    "model = pickle.load(open(model_path + model_name, 'rb'))\n",
    "            \n",
    "res_list = []\n",
    "# predict\n",
    "report_freq = len(session_groups) // 5 \n",
    "count = 0\n",
    "for sid, session in session_groups:\n",
    "    pred_df = predict_function(sid, session, model, item_idx_map, idx_item_map)\n",
    "    res_list.append(pred_df)\n",
    "    # reset states\n",
    "    model.get_layer('gru_left').reset_states()\n",
    "    model.get_layer('gru_right').reset_states()\n",
    "    # print progress\n",
    "    count += 1\n",
    "    if count % report_freq == 0:\n",
    "        print(\"Predicted for \" + str(count) + \" sessions. \" + str(len(session_groups) - count) + \" sessions to go.\" )\n",
    "# concat results\n",
    "res = pd.concat(res_list)\n",
    "res = res.reindex(columns = [\"session_id\", \"input_items\", \"input_count\", \"position\", \"remaining_items\", \"remaining_count\", \"predictions\"])\n",
    "\n",
    "res.to_csv(predict_path + \"test_14d_prnn.csv\", sep='\\t')\n",
    "            \n",
    "print(\"Stored predictions: \" + predict_path + \"test_14d_prnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
