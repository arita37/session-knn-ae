{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Algorithms.baselines as base\n",
    "import Algorithms.context_knn as cknn\n",
    "import Algorithms.SeqContextKNN as scknn\n",
    "import Algorithms.gru4rec as gru4rec\n",
    "import Algorithms.svmknn as svmknn\n",
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBPR(lambda_session,lambda_item):\n",
    "    algo = base.BPR(lambda_session = lambda_session,lambda_item = lambda_item)\n",
    "\n",
    "    print(\"Training algorithm: BPR with lambda_session {} and lambda_item {}\".format(lambda_session,lambda_item))\n",
    "\n",
    "    algo.fit(train)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = \"models/valid/cb12_BPR_ls\"  + str(lambda_session) + 'li'+ str(lambda_item) + \".model\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(\"Finished training. Storing model to: \" + filename)\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(algo,f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIKNN(lmbd,alpha):\n",
    "    algo = base.ItemKNN(lmbd=lmbd, alpha=alpha)\n",
    "\n",
    "    print(\"Training algorithm: ItemKNN with lambda {} and alpha {}\".format(lmbd,alpha))\n",
    "\n",
    "    algo.fit(train)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = \"models/valid/cb12_IKNN_lmbd\" + str(lmbd) + 'alpha' + str(alpha) + \".model\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(\"Finished training. Storing model to: \" + filename)\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(algo,f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSKNN(sampling, similarity, pop_boost):\n",
    "    \"\"\"\n",
    "    Also known as context knn or cknn\n",
    "    k = 500\n",
    "    sampling = 1000\n",
    "    \"\"\"\n",
    "    algo = cknn.ContextKNN(k = 500, sampling=sampling, similarity = similarity, pop_boost = pop_boost)\n",
    "\n",
    "    print(\"Training algorithm: SKNN with sampling {}, similarity {} and pop_boost {}\".format(sampling, similarity,\n",
    "                                                                                                pop_boost))\n",
    "\n",
    "    algo.fit(train)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = \"models/valid/cb12_SKNN_Smpl\" + str(sampling) + 'Sim' + str(similarity) + 'Pop_boost' + str(pop_boost) + \".model\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(\"Finished training. Storing model to: \" + filename)\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(algo,f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSsKNN(sampling,similarity,pop_boost):\n",
    "\n",
    "    \"\"\"\n",
    "    Also known as sequental context KNN\n",
    "    k = 500\n",
    "    sampling = 1000\n",
    "    \"\"\"\n",
    "\n",
    "    algo = scknn.SeqContextKNN(sampling=sampling, similarity = similarity, pop_boost = pop_boost)\n",
    "\n",
    "    print(\"Training algorithm: S-sKNN with sampling {}, similarity {} and pop_boost {}\".format(sampling, similarity,\n",
    "                                                                                                pop_boost))\n",
    "\n",
    "    algo.fit(train)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = \"models/valid/cb12_S-sKNN_Smpl\" + str(sampling) + 'Sim' + str(similarity) + 'Pop_boost' + str(pop_boost) + \".model\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(\"Finished training. Storing model to: \" + filename)\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(algo,f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSVMKNN(sampling,similarity,pop_boost):\n",
    "    \"\"\"\n",
    "    k = 500\n",
    "    sampling = 1000\n",
    "    \"\"\"\n",
    "\n",
    "    algo = svmknn.VMContextKNN(sampling=sampling, similarity = similarity, pop_boost = pop_boost)\n",
    "\n",
    "    print(\"Training algorithm: SVMKNN with sampling {}, similarity {} and pop_boost {}\".format(sampling, similarity,\n",
    "                                                                                                pop_boost))\n",
    "\n",
    "    algo.fit(train)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = \"models/valid/cb12_SVMKNN_Smpl\" + str(sampling) + 'Sim' + str(similarity) + 'Pop_boost' + str(pop_boost) + \".model\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(\"Finished training. Storing model to: \" + filename)\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(algo,f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGRU4Rec(loss, layers):\n",
    "\n",
    "    algo = gru4rec.GRU4Rec(\n",
    "        loss=loss,\n",
    "        final_act='linear', \n",
    "        hidden_act='tanh', \n",
    "        layers=layers, \n",
    "        batch_size=32, \n",
    "        dropout_p_hidden=0.0, \n",
    "        learning_rate=0.2, \n",
    "        momentum=0.5, \n",
    "        n_sample=2048, \n",
    "        sample_alpha=0, \n",
    "        time_sort=True\n",
    "    )\n",
    "\n",
    "    layers_str = '_'.join(str(x) for x in layers)\n",
    "    print(\"Training algorithm: GRU4Rec with loss {} and layers {}\".format(loss, layers_str))\n",
    "\n",
    "    algo.fit(train)\n",
    "\n",
    "    # save the model to disk\n",
    "    filename = \"models/valid/cb12_GRU4Rec_loss\" + loss + '_layers' + layers_str + \".model\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(\"Finished training. Storing model to: \" + filename)\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(algo,f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../data/cb12/processed/valid_train_14d.csv'\n",
    "train = pd.read_csv(train_path, sep='\\t')[['session_id', 'item_id', 'created_at']]\n",
    "train.columns = ['SessionId', 'ItemId', 'Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training BPR models\n",
    "bpr_params = [[0, 0], [0, 0.25], [0, 0.5], [0.25, 0], [0.25, 0.25],[0.25, 0.5], [0.5, 0], [0.5, 0.25], [0.5, 0.5]]\n",
    "for i in range (len(bpr_params)):\n",
    "    trainBPR(bpr_params[i][0], bpr_params[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training IKNN models\n",
    "\n",
    "iknn_params = [[20, 0.25], [20, 0.5], [20, 0.75], [50, 0.25], [50, 0.5], [50, 0.75], [80, 0.25], [80, 0.5], [80, 0.75]]\n",
    "for i in range(len(iknn_params)):\n",
    "    trainIKNN(iknn_params[i][0], iknn_params[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training sKNN\n",
    "\n",
    "sknn_params = [[\"recent\",\"jaccard\",0],[\"recent\",\"jaccard\",1],\n",
    "               [\"recent\", \"cosine\", 0],[\"recent\",\"cosine\",1],\n",
    "               [\"random\",\"jaccard\",0],[\"random\",\"jaccard\",1],\n",
    "               [\"random\",\"cosine\",0],[\"random\",\"cosine\",1]]\n",
    "\n",
    "for i in range (len(sknn_params)):\n",
    "    trainSKNN((sknn_params[i])[0],(sknn_params[i])[1],(sknn_params[i])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVKNN with same params as sknn\n",
    "for i in range(len(sknn_params)):\n",
    "    trainSVMKNN((sknn_params[i])[0],(sknn_params[i])[1],(sknn_params[i])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training S-sknn with same params as sknn\n",
    "\n",
    "for i in range(len(sknn_params)):\n",
    "    trainSsKNN((sknn_params[i])[0], (sknn_params[i])[1], (sknn_params[i])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training GRU4Rec models\n",
    "gru_params = [\n",
    "    [\"top1-max\", [100]], [\"top1-max\", [100,100]], [\"top1-max\", [1000]], [\"top1-max\", [1000,1000]], \n",
    "    [\"bpr-max-0.5\", [100]],[\"bpr-max-0.5\", [100,100]], [\"bpr-max-0.5\", [1000]], [\"bpr-max-0.5\", [1000,1000]]\n",
    "]\n",
    "gru_params = [[\"bpr-max-0.5\", [1000,1000]]]\n",
    "for i in range (len(gru_params)):\n",
    "    trainGRU4Rec(gru_params[i][0], gru_params[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
